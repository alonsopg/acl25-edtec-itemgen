{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b894162",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../../')\n",
    "from evaluator import Evaluator\n",
    "from retriever import *\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from transformers import logging\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbf18eb",
   "metadata": {},
   "source": [
    "## 1. Retrieve & Augment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef5c51b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[API Retriever] Processing query: arbeitssicherheit und gesundheitsschutz\n",
      "[API Retriever] Sending request for query: arbeitssicherheit und gesundheitsschutz\n",
      "[API Retriever] Using KPE: False\n",
      "[API Retriever] Processing query: internet-sicherheit\n",
      "[API Retriever] Sending request for query: internet-sicherheit\n",
      "[API Retriever] Using KPE: False\n",
      "[API Retriever] Processing query: deutsche sprachkompetenz\n",
      "[API Retriever] Sending request for query: deutsche sprachkompetenz\n",
      "[API Retriever] Using KPE: False\n",
      "[API Retriever] Processing query: e-learning und kursbewertung\n",
      "[API Retriever] Sending request for query: e-learning und kursbewertung\n",
      "[API Retriever] Using KPE: False\n",
      "[API Retriever] Processing query: einsatz von technologie\n",
      "[API Retriever] Sending request for query: einsatz von technologie\n",
      "[API Retriever] Using KPE: False\n",
      "[API Retriever] Processing query: allgemeine mathematische kompetenz\n",
      "[API Retriever] Sending request for query: allgemeine mathematische kompetenz\n",
      "[API Retriever] Using KPE: False\n",
      "[API Retriever] Processing query: grundlegende statistiken\n",
      "[API Retriever] Sending request for query: grundlegende statistiken\n",
      "[API Retriever] Using KPE: False\n",
      "[API Retriever] Processing query: aufgabenmanagement\n",
      "[API Retriever] Sending request for query: aufgabenmanagement\n",
      "[API Retriever] Using KPE: False\n",
      "[API Retriever] Processing query: lager und logistik\n",
      "[API Retriever] Sending request for query: lager und logistik\n",
      "[API Retriever] Using KPE: False\n",
      "[API Retriever] Processing query: inhaltserstellung\n",
      "[API Retriever] Sending request for query: inhaltserstellung\n",
      "[API Retriever] Using KPE: False\n",
      "[API Retriever] Search completed.\n"
     ]
    }
   ],
   "source": [
    "api_retriever = APIRetriever(\n",
    "                            api_url=\"http://127.0.0.1:5000/search\",\n",
    "                            api_key=\"\",\n",
    "                            kpe = \"False\",\n",
    "                            llm_version =\"gpt-4o\",\n",
    "                            enable_logging=True\n",
    "                            )\n",
    "\n",
    "queries_df = pd.read_csv(\"../../../question-retrieval-KIPerWeb/testbeds/queries_experiments/queries/queries_2.csv\")\n",
    "questions_df = api_retriever.search_queries(queries_df, k=25)\n",
    "questions_df.to_csv(\"../augmented-generation/results/wo_kpe.csv\", index=False)\n",
    "questions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bbee1c",
   "metadata": {},
   "source": [
    "<!-- https://chatgpt.com/share/67cc5869-0000-800c-9601-9926e877e5ac -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fdec254",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mStarting evaluation for docid: 399\u001b[0m\u001b[92mStarting evaluation for docid: 713\u001b[0m\n",
      "\n",
      "\u001b[92mStarting evaluation for docid: 326\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 853\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 865\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1106\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 398\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 855\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 860\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1101\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 239\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating docs:   0%|                                                                  | 0/240 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.14 sec\u001b[0m\n",
      "\u001b[93mDocid 1101: Non-NLI GPT metrics computed in 1.14 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=16 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.18 sec\u001b[0m\n",
      "\u001b[93mDocid 713: Non-NLI GPT metrics computed in 1.18 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=54 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.24 sec\u001b[0m\n",
      "\u001b[93mDocid 855: Non-NLI GPT metrics computed in 1.24 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=44 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.25 sec\u001b[0m\n",
      "\u001b[93mDocid 239: Non-NLI GPT metrics computed in 1.25 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=22 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.28 sec\u001b[0m\n",
      "\u001b[93mDocid 398: Non-NLI GPT metrics computed in 1.28 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=28 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.38 sec\u001b[0m\n",
      "\u001b[93mDocid 326: Non-NLI GPT metrics computed in 1.38 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=59 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.40 sec\u001b[0m\n",
      "\u001b[93mDocid 399: Non-NLI GPT metrics computed in 1.40 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=35 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.40 sec\u001b[0m\n",
      "\u001b[93mDocid 865: Non-NLI GPT metrics computed in 1.40 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=48 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.45 sec\u001b[0m\n",
      "\u001b[93mDocid 1106: Non-NLI GPT metrics computed in 1.45 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=52 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.51 sec\u001b[0m\n",
      "\u001b[93mDocid 853: Non-NLI GPT metrics computed in 1.51 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=31 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.50 sec\u001b[0m\n",
      "\u001b[93mDocid 860: Non-NLI GPT metrics computed in 1.50 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=34 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:   1%|▏                     | 2/240 [00:02<03:43,  1.06it/s, Last docid=398, Elapsed (min)=0.04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.99 sec\u001b[0m\n",
      "\u001b[95mDocid 1101: Clarity evaluation completed in 0.99 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1101: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1101\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1109\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.98 sec\u001b[0m\n",
      "\u001b[95mDocid 398: Clarity evaluation completed in 0.98 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 398: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 398\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 714\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:   3%|▋                     | 8/240 [00:02<00:37,  6.19it/s, Last docid=860, Elapsed (min)=0.04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.96 sec\u001b[0m\n",
      "\u001b[95mDocid 865: Clarity evaluation completed in 0.96 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 865: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 865\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1108\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.21 sec\u001b[0m\n",
      "\u001b[95mDocid 713: Clarity evaluation completed in 1.21 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 713: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 713\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 956\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.00 sec\u001b[0m\n",
      "\u001b[95mDocid 399: Clarity evaluation completed in 1.00 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 399: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 399\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 863\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.15 sec\u001b[0m\n",
      "\u001b[95mDocid 855: Clarity evaluation completed in 1.15 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 855: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 855\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 864\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.15 sec\u001b[0m\n",
      "\u001b[95mDocid 239: Clarity evaluation completed in 1.15 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 239: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 239\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 961\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.03 sec\u001b[0m\n",
      "\u001b[95mDocid 853: Clarity evaluation completed in 1.03 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 853: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 853\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 397\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.03 sec\u001b[0m\n",
      "\u001b[95mDocid 860: Clarity evaluation completed in 1.03 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 860: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 860\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 404\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating docs:   4%|▊                     | 9/240 [00:02<00:37,  6.19it/s, Last docid=326, Elapsed (min)=0.04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.19 sec\u001b[0m\n",
      "\u001b[95mDocid 326: Clarity evaluation completed in 1.19 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 326: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 326\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 400\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.31 sec\u001b[0m\n",
      "\u001b[95mDocid 1106: Clarity evaluation completed in 1.31 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1106: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1106\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 396\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:   5%|▉                   | 11/240 [00:02<00:29,  7.72it/s, Last docid=1106, Elapsed (min)=0.05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.19 sec\u001b[0m\n",
      "\u001b[93mDocid 1109: Non-NLI GPT metrics computed in 1.19 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=51 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.24 sec\u001b[0m\n",
      "\u001b[93mDocid 714: Non-NLI GPT metrics computed in 1.24 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=45 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.20 sec\u001b[0m\n",
      "\u001b[93mDocid 956: Non-NLI GPT metrics computed in 1.20 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=16 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.20 sec\u001b[0m\n",
      "\u001b[93mDocid 961: Non-NLI GPT metrics computed in 1.20 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=51 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.22 sec\u001b[0m\n",
      "\u001b[93mDocid 864: Non-NLI GPT metrics computed in 1.22 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=44 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.10 sec\u001b[0m\n",
      "\u001b[93mDocid 397: Non-NLI GPT metrics computed in 1.10 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=13 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.56 sec\u001b[0m\n",
      "\u001b[93mDocid 1108: Non-NLI GPT metrics computed in 1.57 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=56 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.40 sec\u001b[0m\n",
      "\u001b[93mDocid 404: Non-NLI GPT metrics computed in 1.40 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=51 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.20 sec\u001b[0m\n",
      "\u001b[93mDocid 396: Non-NLI GPT metrics computed in 1.20 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=19 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.59 sec\u001b[0m\n",
      "\u001b[93mDocid 863: Non-NLI GPT metrics computed in 1.59 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=34 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.45 sec\u001b[0m\n",
      "\u001b[93mDocid 400: Non-NLI GPT metrics computed in 1.45 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=19 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:   5%|█▏                   | 13/240 [00:04<01:17,  2.93it/s, Last docid=961, Elapsed (min)=0.08]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.07 sec\u001b[0m\n",
      "\u001b[95mDocid 1109: Clarity evaluation completed in 1.07 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1109: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1109\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 456\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.93 sec\u001b[0m\n",
      "\u001b[95mDocid 961: Clarity evaluation completed in 0.93 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 961: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 961\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 856\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:   6%|█▎                   | 15/240 [00:04<01:03,  3.55it/s, Last docid=714, Elapsed (min)=0.08]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.05 sec\u001b[0m\n",
      "\u001b[95mDocid 956: Clarity evaluation completed in 1.05 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 956: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 956\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1100\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.29 sec\u001b[0m\n",
      "\u001b[95mDocid 714: Clarity evaluation completed in 1.29 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 714: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 714\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 911\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:   8%|█▌                  | 18/240 [00:05<00:44,  5.04it/s, Last docid=1108, Elapsed (min)=0.09]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.94 sec\u001b[0m\n",
      "\u001b[95mDocid 396: Clarity evaluation completed in 0.94 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 396: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 396\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1010\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.03 sec\u001b[0m\n",
      "\u001b[95mDocid 863: Clarity evaluation completed in 1.03 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 863: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 863\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1021\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.15 sec\u001b[0m\n",
      "\u001b[95mDocid 1108: Clarity evaluation completed in 1.15 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1108: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1108\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 982\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:   8%|█▊                   | 20/240 [00:05<00:33,  6.56it/s, Last docid=864, Elapsed (min)=0.09]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.49 sec\u001b[0m\n",
      "\u001b[95mDocid 397: Clarity evaluation completed in 1.50 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 397: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 397\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 909\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.19 sec\u001b[0m\n",
      "\u001b[95mDocid 400: Clarity evaluation completed in 1.19 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 400: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 400\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 238\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.67 sec\u001b[0m\n",
      "\u001b[95mDocid 864: Clarity evaluation completed in 1.67 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 864: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 864\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1014\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:   9%|█▉                   | 22/240 [00:05<00:35,  6.07it/s, Last docid=404, Elapsed (min)=0.09]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.63 sec\u001b[0m\n",
      "\u001b[95mDocid 404: Clarity evaluation completed in 1.63 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 404: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 404\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1002\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.45 sec\u001b[0m\n",
      "\u001b[93mDocid 456: Non-NLI GPT metrics computed in 1.45 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=93 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.37 sec\u001b[0m\n",
      "\u001b[93mDocid 856: Non-NLI GPT metrics computed in 1.37 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=43 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.30 sec\u001b[0m\n",
      "\u001b[93mDocid 1100: Non-NLI GPT metrics computed in 1.30 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=50 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.00 sec\u001b[0m\n",
      "\u001b[93mDocid 238: Non-NLI GPT metrics computed in 1.00 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=28 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.44 sec\u001b[0m\n",
      "\u001b[93mDocid 911: Non-NLI GPT metrics computed in 1.44 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=35 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.24 sec\u001b[0m\n",
      "\u001b[93mDocid 1021: Non-NLI GPT metrics computed in 1.24 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=34 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.27 sec\u001b[0m\n",
      "\u001b[93mDocid 982: Non-NLI GPT metrics computed in 1.27 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=33 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.57 sec\u001b[0m\n",
      "\u001b[93mDocid 1010: Non-NLI GPT metrics computed in 1.57 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=35 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.28 sec\u001b[0m\n",
      "\u001b[93mDocid 1014: Non-NLI GPT metrics computed in 1.28 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=28 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.65 sec\u001b[0m\n",
      "\u001b[93mDocid 909: Non-NLI GPT metrics computed in 1.65 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=33 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.37 sec\u001b[0m\n",
      "\u001b[93mDocid 1002: Non-NLI GPT metrics computed in 1.37 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=42 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  10%|██▏                  | 25/240 [00:07<01:14,  2.87it/s, Last docid=982, Elapsed (min)=0.12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.09 sec\u001b[0m\n",
      "\u001b[95mDocid 1100: Clarity evaluation completed in 1.09 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1100: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1100\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 991\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.18 sec\u001b[0m\n",
      "\u001b[95mDocid 856: Clarity evaluation completed in 1.18 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 856: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 856\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1001\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.92 sec\u001b[0m\n",
      "\u001b[95mDocid 911: Clarity evaluation completed in 0.92 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 911: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 911\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 980\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.83 sec\u001b[0m\n",
      "\u001b[95mDocid 982: Clarity evaluation completed in 0.83 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 982: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 982\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1105\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  11%|██▎                  | 27/240 [00:07<00:49,  4.28it/s, Last docid=238, Elapsed (min)=0.12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.41 sec\u001b[0m\n",
      "\u001b[95mDocid 456: Clarity evaluation completed in 1.41 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 456: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 456\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 286\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.16 sec\u001b[0m\n",
      "\u001b[95mDocid 238: Clarity evaluation completed in 1.16 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 238: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 238\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 861\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  12%|██▍                 | 29/240 [00:07<00:44,  4.72it/s, Last docid=1014, Elapsed (min)=0.13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.80 sec\u001b[0m\n",
      "\u001b[95mDocid 909: Clarity evaluation completed in 0.80 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 909: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 909\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 27\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.03 sec\u001b[0m\n",
      "\u001b[95mDocid 1014: Clarity evaluation completed in 1.03 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1014: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1014\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 560\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  13%|██▋                 | 32/240 [00:07<00:41,  5.01it/s, Last docid=1010, Elapsed (min)=0.13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.94 sec\u001b[0m\n",
      "\u001b[95mDocid 1002: Clarity evaluation completed in 0.94 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1002: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1002\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 90\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.67 sec\u001b[0m\n",
      "\u001b[95mDocid 1021: Clarity evaluation completed in 1.67 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1021: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1021\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 278\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.47 sec\u001b[0m\n",
      "\u001b[95mDocid 1010: Clarity evaluation completed in 1.47 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1010: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1010\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 989\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.00 sec\u001b[0m\n",
      "\u001b[93mDocid 991: Non-NLI GPT metrics computed in 1.00 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=26 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.24 sec\u001b[0m\n",
      "\u001b[93mDocid 980: Non-NLI GPT metrics computed in 1.24 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=20 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.06 sec\u001b[0m\n",
      "\u001b[93mDocid 861: Non-NLI GPT metrics computed in 1.06 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=36 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.27 sec\u001b[0m\n",
      "\u001b[93mDocid 1105: Non-NLI GPT metrics computed in 1.27 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=47 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.39 sec\u001b[0m\n",
      "\u001b[93mDocid 1001: Non-NLI GPT metrics computed in 1.39 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=40 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.91 sec\u001b[0m\n",
      "\u001b[93mDocid 27: Non-NLI GPT metrics computed in 0.91 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=22 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.28 sec\u001b[0m\n",
      "\u001b[93mDocid 286: Non-NLI GPT metrics computed in 1.28 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=31 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  14%|██▉                  | 34/240 [00:08<00:49,  4.13it/s, Last docid=991, Elapsed (min)=0.15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.82 sec\u001b[0m\n",
      "\u001b[95mDocid 991: Clarity evaluation completed in 0.82 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 991: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 991\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 962\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.15 sec\u001b[0m\n",
      "\u001b[93mDocid 90: Non-NLI GPT metrics computed in 1.15 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=14 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.50 sec\u001b[0m\n",
      "\u001b[93mDocid 560: Non-NLI GPT metrics computed in 1.50 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=38 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  15%|███                  | 35/240 [00:09<00:59,  3.44it/s, Last docid=286, Elapsed (min)=0.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.40 sec\u001b[0m\n",
      "\u001b[93mDocid 278: Non-NLI GPT metrics computed in 1.40 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=57 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.00 sec\u001b[0m\n",
      "\u001b[95mDocid 980: Clarity evaluation completed in 1.00 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 980: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 980\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 712\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.88 sec\u001b[0m\n",
      "\u001b[95mDocid 286: Clarity evaluation completed in 0.88 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 286: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 286\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 995\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.04 sec\u001b[0m\n",
      "\u001b[95mDocid 27: Clarity evaluation completed in 1.04 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 27: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 27\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 718\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  15%|███▍                  | 37/240 [00:09<00:45,  4.51it/s, Last docid=27, Elapsed (min)=0.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.61 sec\u001b[0m\n",
      "\u001b[93mDocid 989: Non-NLI GPT metrics computed in 1.61 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=32 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  16%|███▏                | 38/240 [00:09<00:47,  4.29it/s, Last docid=1105, Elapsed (min)=0.17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.39 sec\u001b[0m\n",
      "\u001b[95mDocid 861: Clarity evaluation completed in 1.39 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 861: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 861\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 677\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.42 sec\u001b[0m\n",
      "\u001b[95mDocid 1105: Clarity evaluation completed in 1.42 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1105: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1105\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 517\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.03 sec\u001b[0m\n",
      "\u001b[93mDocid 962: Non-NLI GPT metrics computed in 1.03 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=34 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.54 sec\u001b[0m\n",
      "\u001b[95mDocid 1001: Clarity evaluation completed in 1.54 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1001: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1001\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 357\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  18%|███▊                  | 42/240 [00:10<00:31,  6.28it/s, Last docid=90, Elapsed (min)=0.17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.02 sec\u001b[0m\n",
      "\u001b[95mDocid 560: Clarity evaluation completed in 1.02 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 560: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 560\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 292\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.17 sec\u001b[0m\n",
      "\u001b[95mDocid 90: Clarity evaluation completed in 1.17 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 90: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 90\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 362\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  18%|███▊                 | 43/240 [00:10<00:32,  6.10it/s, Last docid=989, Elapsed (min)=0.18]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.07 sec\u001b[0m\n",
      "\u001b[95mDocid 278: Clarity evaluation completed in 1.07 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 278: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 278\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 361\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.94 sec\u001b[0m\n",
      "\u001b[95mDocid 989: Clarity evaluation completed in 0.94 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 989: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 989\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 358\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.02 sec\u001b[0m\n",
      "\u001b[93mDocid 718: Non-NLI GPT metrics computed in 1.02 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=53 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  19%|███▉                 | 45/240 [00:10<00:30,  6.50it/s, Last docid=962, Elapsed (min)=0.18]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.22 sec\u001b[0m\n",
      "\u001b[93mDocid 712: Non-NLI GPT metrics computed in 1.22 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=61 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.78 sec\u001b[0m\n",
      "\u001b[95mDocid 962: Clarity evaluation completed in 0.78 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 962: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 962\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 215\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.93 sec\u001b[0m\n",
      "\u001b[93mDocid 517: Non-NLI GPT metrics computed in 0.93 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=19 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.55 sec\u001b[0m\n",
      "\u001b[93mDocid 995: Non-NLI GPT metrics computed in 1.55 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=26 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.21 sec\u001b[0m\n",
      "\u001b[93mDocid 677: Non-NLI GPT metrics computed in 1.21 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=14 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.85 sec\u001b[0m\n",
      "\u001b[93mDocid 362: Non-NLI GPT metrics computed in 0.85 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=22 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.12 sec\u001b[0m\n",
      "\u001b[93mDocid 357: Non-NLI GPT metrics computed in 1.12 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=27 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.11 sec\u001b[0m\n",
      "\u001b[93mDocid 292: Non-NLI GPT metrics computed in 1.11 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=39 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.79 sec\u001b[0m\n",
      "\u001b[93mDocid 358: Non-NLI GPT metrics computed in 0.79 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=14 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  19%|████                 | 46/240 [00:11<00:55,  3.52it/s, Last docid=718, Elapsed (min)=0.19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.91 sec\u001b[0m\n",
      "\u001b[95mDocid 718: Clarity evaluation completed in 0.91 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 718: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 718\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 439\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.18 sec\u001b[0m\n",
      "\u001b[93mDocid 361: Non-NLI GPT metrics computed in 1.18 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=14 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.95 sec\u001b[0m\n",
      "\u001b[93mDocid 215: Non-NLI GPT metrics computed in 0.95 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=20 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  20%|████                 | 47/240 [00:11<00:54,  3.53it/s, Last docid=995, Elapsed (min)=0.20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.12 sec\u001b[0m\n",
      "\u001b[95mDocid 712: Clarity evaluation completed in 1.13 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 712: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 712\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 518\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.84 sec\u001b[0m\n",
      "\u001b[95mDocid 995: Clarity evaluation completed in 0.84 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 995: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 995\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 368\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  20%|████▎                | 49/240 [00:12<00:46,  4.07it/s, Last docid=357, Elapsed (min)=0.20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.21 sec\u001b[0m\n",
      "\u001b[95mDocid 517: Clarity evaluation completed in 1.21 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 517: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 517\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 479\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.00 sec\u001b[0m\n",
      "\u001b[95mDocid 357: Clarity evaluation completed in 1.00 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 357: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 357\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 420\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  23%|████▊                | 55/240 [00:12<00:23,  7.99it/s, Last docid=215, Elapsed (min)=0.21]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.10 sec\u001b[0m\n",
      "\u001b[95mDocid 358: Clarity evaluation completed in 1.10 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 358: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 358\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 209\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.34 sec\u001b[0m\n",
      "\u001b[95mDocid 362: Clarity evaluation completed in 1.34 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 362: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 362\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 355\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.20 sec\u001b[0m\n",
      "\u001b[95mDocid 292: Clarity evaluation completed in 1.20 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 292: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 292\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 214\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.86 sec\u001b[0m\n",
      "\u001b[95mDocid 361: Clarity evaluation completed in 0.86 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 361: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 361\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 448\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.95 sec\u001b[0m\n",
      "\u001b[95mDocid 215: Clarity evaluation completed in 0.95 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 215: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 215\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 489\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.93 sec\u001b[0m\n",
      "\u001b[93mDocid 518: Non-NLI GPT metrics computed in 0.93 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=22 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.23 sec\u001b[0m\n",
      "\u001b[93mDocid 439: Non-NLI GPT metrics computed in 1.23 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=56 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.98 sec\u001b[0m\n",
      "\u001b[93mDocid 368: Non-NLI GPT metrics computed in 0.98 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=14 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.96 sec\u001b[0m\n",
      "\u001b[93mDocid 479: Non-NLI GPT metrics computed in 0.96 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=20 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.82 sec\u001b[0m\n",
      "\u001b[93mDocid 355: Non-NLI GPT metrics computed in 0.82 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=14 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.11 sec\u001b[0m\n",
      "\u001b[93mDocid 420: Non-NLI GPT metrics computed in 1.11 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=32 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  24%|████▉                | 57/240 [00:13<00:44,  4.07it/s, Last docid=518, Elapsed (min)=0.23]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.16 sec\u001b[0m\n",
      "\u001b[93mDocid 209: Non-NLI GPT metrics computed in 1.16 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=16 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.13 sec\u001b[0m\n",
      "\u001b[93mDocid 214: Non-NLI GPT metrics computed in 1.13 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=9 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.14 sec\u001b[0m\n",
      "\u001b[93mDocid 448: Non-NLI GPT metrics computed in 1.14 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=27 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.92 sec\u001b[0m\n",
      "\u001b[95mDocid 439: Clarity evaluation completed in 0.92 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 439: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 439\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 125\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.04 sec\u001b[0m\n",
      "\u001b[95mDocid 518: Clarity evaluation completed in 1.04 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 518: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 518\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 422\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.17 sec\u001b[0m\n",
      "\u001b[93mDocid 489: Non-NLI GPT metrics computed in 1.17 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=19 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  24%|█████                | 58/240 [00:13<00:41,  4.35it/s, Last docid=479, Elapsed (min)=0.23]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 2.85 sec\u001b[0m\n",
      "\u001b[95mDocid 677: Clarity evaluation completed in 2.85 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 677: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 677\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 514\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.96 sec\u001b[0m\n",
      "\u001b[95mDocid 479: Clarity evaluation completed in 0.96 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 479: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 479\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 447\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  25%|█████▎               | 61/240 [00:14<00:34,  5.21it/s, Last docid=214, Elapsed (min)=0.24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.94 sec\u001b[0m\n",
      "\u001b[95mDocid 355: Clarity evaluation completed in 0.94 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 355: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 355\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 226\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.06 sec\u001b[0m\n",
      "\u001b[95mDocid 420: Clarity evaluation completed in 1.06 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 420: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 420\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 360\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.81 sec\u001b[0m\n",
      "\u001b[95mDocid 214: Clarity evaluation completed in 0.81 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 214: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 214\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 206\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  26%|█████▌               | 63/240 [00:14<00:25,  6.95it/s, Last docid=448, Elapsed (min)=0.24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.88 sec\u001b[0m\n",
      "\u001b[95mDocid 209: Clarity evaluation completed in 0.88 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 209: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 209\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 671\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.02 sec\u001b[0m\n",
      "\u001b[95mDocid 448: Clarity evaluation completed in 1.02 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 448: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 448\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 939\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating docs:  27%|█████▌               | 64/240 [00:15<00:25,  6.95it/s, Last docid=489, Elapsed (min)=0.25]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.18 sec\u001b[0m\n",
      "\u001b[93mDocid 422: Non-NLI GPT metrics computed in 1.18 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=59 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.05 sec\u001b[0m\n",
      "\u001b[93mDocid 514: Non-NLI GPT metrics computed in 1.05 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=19 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.35 sec\u001b[0m\n",
      "\u001b[95mDocid 489: Clarity evaluation completed in 1.35 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 489: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 489\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 938\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating docs:  27%|█████▋               | 65/240 [00:15<00:36,  4.75it/s, Last docid=489, Elapsed (min)=0.25]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.27 sec\u001b[0m\n",
      "\u001b[93mDocid 447: Non-NLI GPT metrics computed in 1.27 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=34 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.85 sec\u001b[0m\n",
      "\u001b[93mDocid 206: Non-NLI GPT metrics computed in 0.85 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=20 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.06 sec\u001b[0m\n",
      "\u001b[93mDocid 226: Non-NLI GPT metrics computed in 1.06 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=25 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.19 sec\u001b[0m\n",
      "\u001b[93mDocid 360: Non-NLI GPT metrics computed in 1.19 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=27 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.10 sec\u001b[0m\n",
      "\u001b[93mDocid 671: Non-NLI GPT metrics computed in 1.10 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=17 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  28%|█████▊               | 66/240 [00:15<00:52,  3.29it/s, Last docid=368, Elapsed (min)=0.26]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 3.00 sec\u001b[0m\n",
      "\u001b[95mDocid 368: Clarity evaluation completed in 3.01 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 368: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 368\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 455\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.32 sec\u001b[0m\n",
      "\u001b[93mDocid 939: Non-NLI GPT metrics computed in 1.32 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=29 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  29%|██████               | 69/240 [00:16<00:35,  4.80it/s, Last docid=422, Elapsed (min)=0.27]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.09 sec\u001b[0m\n",
      "\u001b[95mDocid 514: Clarity evaluation completed in 1.09 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 514: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 514\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1024\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.82 sec\u001b[0m\n",
      "\u001b[95mDocid 206: Clarity evaluation completed in 0.82 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 206: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 206\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 662\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.27 sec\u001b[0m\n",
      "\u001b[95mDocid 422: Clarity evaluation completed in 1.27 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 422: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 422\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 979\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 2.58 sec\u001b[0m\n",
      "\u001b[93mDocid 125: Non-NLI GPT metrics computed in 2.58 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=19 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.01 sec\u001b[0m\n",
      "\u001b[95mDocid 447: Clarity evaluation completed in 1.01 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 447: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 447\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 707\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  30%|██████▏              | 71/240 [00:16<00:29,  5.71it/s, Last docid=671, Elapsed (min)=0.27]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.13 sec\u001b[0m\n",
      "\u001b[93mDocid 938: Non-NLI GPT metrics computed in 1.13 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=26 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.89 sec\u001b[0m\n",
      "\u001b[95mDocid 671: Clarity evaluation completed in 0.89 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 671: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 671\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 687\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  30%|██████▎              | 72/240 [00:16<00:29,  5.71it/s, Last docid=226, Elapsed (min)=0.28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.95 sec\u001b[0m\n",
      "\u001b[95mDocid 360: Clarity evaluation completed in 0.95 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 360: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 360\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 527\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.23 sec\u001b[0m\n",
      "\u001b[95mDocid 226: Clarity evaluation completed in 1.23 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 226: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 226\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 541\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  31%|██████▍              | 74/240 [00:16<00:26,  6.30it/s, Last docid=939, Elapsed (min)=0.28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.88 sec\u001b[0m\n",
      "\u001b[95mDocid 939: Clarity evaluation completed in 0.88 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 939: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 939\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 680\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.83 sec\u001b[0m\n",
      "\u001b[93mDocid 662: Non-NLI GPT metrics computed in 0.83 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=13 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  31%|██████▌              | 75/240 [00:17<00:30,  5.37it/s, Last docid=938, Elapsed (min)=0.29]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.27 sec\u001b[0m\n",
      "\u001b[93mDocid 455: Non-NLI GPT metrics computed in 1.28 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=23 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.93 sec\u001b[0m\n",
      "\u001b[95mDocid 125: Clarity evaluation completed in 0.93 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 125: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 125\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1022\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.96 sec\u001b[0m\n",
      "\u001b[95mDocid 938: Clarity evaluation completed in 0.96 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 938: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 938\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 703\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.23 sec\u001b[0m\n",
      "\u001b[93mDocid 1024: Non-NLI GPT metrics computed in 1.23 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=19 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.17 sec\u001b[0m\n",
      "\u001b[93mDocid 707: Non-NLI GPT metrics computed in 1.17 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=29 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.99 sec\u001b[0m\n",
      "\u001b[93mDocid 687: Non-NLI GPT metrics computed in 0.99 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=14 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.15 sec\u001b[0m\n",
      "\u001b[93mDocid 527: Non-NLI GPT metrics computed in 1.15 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=19 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  32%|██████▋              | 77/240 [00:17<00:34,  4.71it/s, Last docid=662, Elapsed (min)=0.30]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.78 sec\u001b[0m\n",
      "\u001b[95mDocid 662: Clarity evaluation completed in 0.78 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 662: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 662\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 845\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.90 sec\u001b[0m\n",
      "\u001b[93mDocid 680: Non-NLI GPT metrics computed in 0.90 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=20 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.32 sec\u001b[0m\n",
      "\u001b[93mDocid 541: Non-NLI GPT metrics computed in 1.32 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=10 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  32%|██████▊              | 78/240 [00:18<00:37,  4.29it/s, Last docid=455, Elapsed (min)=0.30]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.91 sec\u001b[0m\n",
      "\u001b[95mDocid 455: Clarity evaluation completed in 0.91 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 455: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 455\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 719\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.04 sec\u001b[0m\n",
      "\u001b[93mDocid 1022: Non-NLI GPT metrics computed in 1.04 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=52 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  33%|██████▉              | 79/240 [00:18<00:40,  3.94it/s, Last docid=707, Elapsed (min)=0.31]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.93 sec\u001b[0m\n",
      "\u001b[95mDocid 707: Clarity evaluation completed in 0.93 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 707: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 707\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 528\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 2.30 sec\u001b[0m\n",
      "\u001b[93mDocid 979: Non-NLI GPT metrics computed in 2.30 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=33 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  33%|███████              | 80/240 [00:18<00:43,  3.64it/s, Last docid=680, Elapsed (min)=0.31]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.41 sec\u001b[0m\n",
      "\u001b[95mDocid 1024: Clarity evaluation completed in 1.41 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1024: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1024\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 841\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.47 sec\u001b[0m\n",
      "\u001b[93mDocid 703: Non-NLI GPT metrics computed in 1.47 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=18 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.98 sec\u001b[0m\n",
      "\u001b[95mDocid 680: Clarity evaluation completed in 0.98 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 680: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 680\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 657\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  35%|███████▎             | 83/240 [00:18<00:32,  4.82it/s, Last docid=527, Elapsed (min)=0.32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.09 sec\u001b[0m\n",
      "\u001b[95mDocid 541: Clarity evaluation completed in 1.09 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 541: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 541\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 977\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.49 sec\u001b[0m\n",
      "\u001b[95mDocid 687: Clarity evaluation completed in 1.49 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 687: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 687\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 665\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.32 sec\u001b[0m\n",
      "\u001b[95mDocid 527: Clarity evaluation completed in 1.32 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 527: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 527\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 720\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.25 sec\u001b[0m\n",
      "\u001b[93mDocid 845: Non-NLI GPT metrics computed in 1.25 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=30 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.95 sec\u001b[0m\n",
      "\u001b[93mDocid 719: Non-NLI GPT metrics computed in 0.95 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=10 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  35%|███████             | 85/240 [00:19<00:25,  6.07it/s, Last docid=1022, Elapsed (min)=0.32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.04 sec\u001b[0m\n",
      "\u001b[95mDocid 1022: Clarity evaluation completed in 1.04 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1022: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1022\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 540\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  36%|███████▌             | 86/240 [00:19<00:30,  5.00it/s, Last docid=979, Elapsed (min)=0.33]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.14 sec\u001b[0m\n",
      "\u001b[93mDocid 528: Non-NLI GPT metrics computed in 1.14 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=27 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.10 sec\u001b[0m\n",
      "\u001b[95mDocid 979: Clarity evaluation completed in 1.10 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 979: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 979\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 722\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  36%|███████▌             | 87/240 [00:19<00:31,  4.85it/s, Last docid=845, Elapsed (min)=0.33]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.13 sec\u001b[0m\n",
      "\u001b[95mDocid 703: Clarity evaluation completed in 1.13 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 703: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 703\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 708\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.92 sec\u001b[0m\n",
      "\u001b[95mDocid 845: Clarity evaluation completed in 0.92 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 845: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 845\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 282\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.42 sec\u001b[0m\n",
      "\u001b[93mDocid 841: Non-NLI GPT metrics computed in 1.42 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=32 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.20 sec\u001b[0m\n",
      "\u001b[93mDocid 665: Non-NLI GPT metrics computed in 1.20 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=13 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.50 sec\u001b[0m\n",
      "\u001b[93mDocid 977: Non-NLI GPT metrics computed in 1.50 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=42 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.24 sec\u001b[0m\n",
      "\u001b[93mDocid 540: Non-NLI GPT metrics computed in 1.24 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=25 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  37%|███████▊             | 89/240 [00:20<00:43,  3.49it/s, Last docid=719, Elapsed (min)=0.35]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.72 sec\u001b[0m\n",
      "\u001b[93mDocid 720: Non-NLI GPT metrics computed in 1.72 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=12 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.72 sec\u001b[0m\n",
      "\u001b[95mDocid 719: Clarity evaluation completed in 1.72 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 719: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 719\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 280\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.86 sec\u001b[0m\n",
      "\u001b[93mDocid 282: Non-NLI GPT metrics computed in 0.86 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=39 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.18 sec\u001b[0m\n",
      "\u001b[93mDocid 722: Non-NLI GPT metrics computed in 1.18 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=16 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  38%|███████▉             | 90/240 [00:20<00:41,  3.65it/s, Last docid=528, Elapsed (min)=0.35]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.41 sec\u001b[0m\n",
      "\u001b[95mDocid 528: Clarity evaluation completed in 1.41 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 528: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 528\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 26\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 2.25 sec\u001b[0m\n",
      "\u001b[93mDocid 657: Non-NLI GPT metrics computed in 2.25 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=17 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  38%|████████             | 92/240 [00:21<00:36,  4.03it/s, Last docid=665, Elapsed (min)=0.36]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.16 sec\u001b[0m\n",
      "\u001b[95mDocid 841: Clarity evaluation completed in 1.16 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 841: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 841\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 30\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.24 sec\u001b[0m\n",
      "\u001b[95mDocid 665: Clarity evaluation completed in 1.24 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 665: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 665\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 334\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  39%|████████▏            | 94/240 [00:21<00:28,  5.15it/s, Last docid=977, Elapsed (min)=0.36]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.06 sec\u001b[0m\n",
      "\u001b[95mDocid 540: Clarity evaluation completed in 1.06 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 540: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 540\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 281\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.25 sec\u001b[0m\n",
      "\u001b[95mDocid 977: Clarity evaluation completed in 1.25 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 977: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 977\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 984\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  40%|████████▍            | 97/240 [00:21<00:17,  8.31it/s, Last docid=722, Elapsed (min)=0.37]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.02 sec\u001b[0m\n",
      "\u001b[95mDocid 282: Clarity evaluation completed in 1.02 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 282: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 282\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 28\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.18 sec\u001b[0m\n",
      "\u001b[95mDocid 720: Clarity evaluation completed in 1.18 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 720: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 720\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 272\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 2.03 sec\u001b[0m\n",
      "\u001b[93mDocid 708: Non-NLI GPT metrics computed in 2.03 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=24 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.09 sec\u001b[0m\n",
      "\u001b[95mDocid 722: Clarity evaluation completed in 1.09 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 722: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 722\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 25\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.01 sec\u001b[0m\n",
      "\u001b[93mDocid 26: Non-NLI GPT metrics computed in 1.01 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=36 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating docs:  40%|████████▍            | 97/240 [00:22<00:17,  8.31it/s, Last docid=657, Elapsed (min)=0.37]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 0.75 sec\u001b[0m\n",
      "\u001b[93mDocid 30: Non-NLI GPT metrics computed in 0.75 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=18 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.36 sec\u001b[0m\n",
      "\u001b[93mDocid 280: Non-NLI GPT metrics computed in 1.36 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=37 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.14 sec\u001b[0m\n",
      "\u001b[95mDocid 657: Clarity evaluation completed in 1.14 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 657: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 657\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 287\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating docs:  41%|████████▌            | 98/240 [00:22<00:17,  8.31it/s, Last docid=708, Elapsed (min)=0.38]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.11 sec\u001b[0m\n",
      "\u001b[93mDocid 281: Non-NLI GPT metrics computed in 1.11 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=53 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.40 sec\u001b[0m\n",
      "\u001b[93mDocid 334: Non-NLI GPT metrics computed in 1.40 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=48 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.97 sec\u001b[0m\n",
      "\u001b[95mDocid 708: Clarity evaluation completed in 0.97 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 708: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 708\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 333\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  42%|████████▍           | 101/240 [00:23<00:29,  4.66it/s, Last docid=280, Elapsed (min)=0.39]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.01 sec\u001b[0m\n",
      "\u001b[95mDocid 26: Clarity evaluation completed in 1.01 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 26: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 26\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 608\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.17 sec\u001b[0m\n",
      "\u001b[93mDocid 272: Non-NLI GPT metrics computed in 1.17 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=21 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.15 sec\u001b[0m\n",
      "\u001b[93mDocid 25: Non-NLI GPT metrics computed in 1.15 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=29 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.28 sec\u001b[0m\n",
      "\u001b[93mDocid 28: Non-NLI GPT metrics computed in 1.29 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=32 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.42 sec\u001b[0m\n",
      "\u001b[93mDocid 984: Non-NLI GPT metrics computed in 1.42 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=10 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.05 sec\u001b[0m\n",
      "\u001b[95mDocid 280: Clarity evaluation completed in 1.05 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 280: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 280\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 32\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  43%|████████▌           | 103/240 [00:23<00:35,  3.85it/s, Last docid=281, Elapsed (min)=0.39]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.76 sec\u001b[0m\n",
      "\u001b[95mDocid 334: Clarity evaluation completed in 0.76 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 334: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 334\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 986\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.57 sec\u001b[0m\n",
      "\u001b[95mDocid 30: Clarity evaluation completed in 1.57 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 30: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 30\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 284\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.98 sec\u001b[0m\n",
      "\u001b[95mDocid 281: Clarity evaluation completed in 0.98 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 281: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 281\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1009\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.55 sec\u001b[0m\n",
      "\u001b[93mDocid 287: Non-NLI GPT metrics computed in 1.55 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=56 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  45%|█████████▎           | 107/240 [00:24<00:22,  5.81it/s, Last docid=28, Elapsed (min)=0.40]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.98 sec\u001b[0m\n",
      "\u001b[95mDocid 272: Clarity evaluation completed in 0.98 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 272: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 272\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 23\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.97 sec\u001b[0m\n",
      "\u001b[95mDocid 984: Clarity evaluation completed in 0.97 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 984: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 984\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 917\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.17 sec\u001b[0m\n",
      "\u001b[93mDocid 608: Non-NLI GPT metrics computed in 1.17 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=44 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.13 sec\u001b[0m\n",
      "\u001b[95mDocid 28: Clarity evaluation completed in 1.13 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 28: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 28\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 907\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  45%|█████████▍           | 108/240 [00:24<00:23,  5.59it/s, Last docid=25, Elapsed (min)=0.41]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.34 sec\u001b[0m\n",
      "\u001b[95mDocid 25: Clarity evaluation completed in 1.34 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 25: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 25\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 219\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.31 sec\u001b[0m\n",
      "\u001b[93mDocid 32: Non-NLI GPT metrics computed in 1.31 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=18 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating docs:  45%|█████████           | 108/240 [00:25<00:23,  5.59it/s, Last docid=608, Elapsed (min)=0.42]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 2.04 sec\u001b[0m\n",
      "\u001b[93mDocid 333: Non-NLI GPT metrics computed in 2.04 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=11 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.34 sec\u001b[0m\n",
      "\u001b[93mDocid 1009: Non-NLI GPT metrics computed in 1.34 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=29 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.39 sec\u001b[0m\n",
      "\u001b[93mDocid 284: Non-NLI GPT metrics computed in 1.39 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=22 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.97 sec\u001b[0m\n",
      "\u001b[95mDocid 608: Clarity evaluation completed in 0.97 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 608: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 608\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 249\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating docs:  45%|█████████           | 109/240 [00:25<00:37,  3.49it/s, Last docid=608, Elapsed (min)=0.42]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.06 sec\u001b[0m\n",
      "\u001b[93mDocid 917: Non-NLI GPT metrics computed in 1.06 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=17 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  46%|█████████▋           | 110/240 [00:25<00:42,  3.04it/s, Last docid=32, Elapsed (min)=0.43]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.12 sec\u001b[0m\n",
      "\u001b[95mDocid 32: Clarity evaluation completed in 1.13 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 32: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 32\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 246\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.57 sec\u001b[0m\n",
      "\u001b[93mDocid 23: Non-NLI GPT metrics computed in 1.57 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=20 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.25 sec\u001b[0m\n",
      "\u001b[93mDocid 219: Non-NLI GPT metrics computed in 1.25 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=13 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  46%|█████████▎          | 111/240 [00:25<00:39,  3.30it/s, Last docid=917, Elapsed (min)=0.43]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.88 sec\u001b[0m\n",
      "\u001b[95mDocid 333: Clarity evaluation completed in 0.88 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 333: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 333\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1049\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.64 sec\u001b[0m\n",
      "\u001b[93mDocid 907: Non-NLI GPT metrics computed in 1.64 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=17 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.70 sec\u001b[0m\n",
      "\u001b[95mDocid 917: Clarity evaluation completed in 0.70 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 917: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 917\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 780\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  47%|████████▉          | 113/240 [00:26<00:28,  4.39it/s, Last docid=1009, Elapsed (min)=0.43]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.04 sec\u001b[0m\n",
      "\u001b[95mDocid 1009: Clarity evaluation completed in 1.04 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1009: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1009\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 790\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.05 sec\u001b[0m\n",
      "\u001b[93mDocid 249: Non-NLI GPT metrics computed in 1.05 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=33 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  48%|█████████▌          | 114/240 [00:26<00:36,  3.50it/s, Last docid=287, Elapsed (min)=0.44]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.51 sec\u001b[0m\n",
      "\u001b[95mDocid 284: Clarity evaluation completed in 1.51 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 284: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 284\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 866\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 2.85 sec\u001b[0m\n",
      "\u001b[95mDocid 287: Clarity evaluation completed in 2.85 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 287: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 287\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 549\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.00 sec\u001b[0m\n",
      "\u001b[93mDocid 246: Non-NLI GPT metrics computed in 1.00 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=36 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.08 sec\u001b[0m\n",
      "\u001b[95mDocid 219: Clarity evaluation completed in 1.08 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 219: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 219\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 867\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  48%|█████████▋          | 116/240 [00:26<00:26,  4.64it/s, Last docid=907, Elapsed (min)=0.45]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.96 sec\u001b[0m\n",
      "\u001b[95mDocid 907: Clarity evaluation completed in 0.96 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 907: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 907\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 874\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  49%|█████████▊          | 118/240 [00:27<00:25,  4.71it/s, Last docid=249, Elapsed (min)=0.45]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.09 sec\u001b[0m\n",
      "\u001b[93mDocid 790: Non-NLI GPT metrics computed in 1.09 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=14 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.31 sec\u001b[0m\n",
      "\u001b[93mDocid 780: Non-NLI GPT metrics computed in 1.31 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=13 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.00 sec\u001b[0m\n",
      "\u001b[95mDocid 249: Clarity evaluation completed in 1.00 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 249: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 249\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 243\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 3.75 sec\u001b[0m\n",
      "\u001b[93mDocid 986: Non-NLI GPT metrics computed in 3.75 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=24 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  50%|██████████          | 120/240 [00:27<00:24,  4.83it/s, Last docid=246, Elapsed (min)=0.46]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.82 sec\u001b[0m\n",
      "\u001b[95mDocid 23: Clarity evaluation completed in 1.82 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 23: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 23\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 410\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.64 sec\u001b[0m\n",
      "\u001b[93mDocid 1049: Non-NLI GPT metrics computed in 1.64 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=22 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.99 sec\u001b[0m\n",
      "\u001b[95mDocid 246: Clarity evaluation completed in 0.99 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 246: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 246\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 241\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.96 sec\u001b[0m\n",
      "\u001b[93mDocid 867: Non-NLI GPT metrics computed in 0.96 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=37 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.17 sec\u001b[0m\n",
      "\u001b[93mDocid 549: Non-NLI GPT metrics computed in 1.17 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=28 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.26 sec\u001b[0m\n",
      "\u001b[93mDocid 866: Non-NLI GPT metrics computed in 1.26 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=15 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  50%|██████████          | 121/240 [00:28<00:33,  3.58it/s, Last docid=780, Elapsed (min)=0.47]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.26 sec\u001b[0m\n",
      "\u001b[93mDocid 874: Non-NLI GPT metrics computed in 1.26 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=49 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.92 sec\u001b[0m\n",
      "\u001b[95mDocid 780: Clarity evaluation completed in 0.92 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 780: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 780\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1035\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  51%|█████████▋         | 122/240 [00:28<00:31,  3.70it/s, Last docid=1049, Elapsed (min)=0.47]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.12 sec\u001b[0m\n",
      "\u001b[93mDocid 243: Non-NLI GPT metrics computed in 1.12 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=14 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.87 sec\u001b[0m\n",
      "\u001b[95mDocid 1049: Clarity evaluation completed in 0.87 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1049: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1049\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 781\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  52%|██████████▎         | 124/240 [00:28<00:24,  4.71it/s, Last docid=867, Elapsed (min)=0.48]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.80 sec\u001b[0m\n",
      "\u001b[95mDocid 549: Clarity evaluation completed in 0.80 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 549: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 549\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 782\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.32 sec\u001b[0m\n",
      "\u001b[95mDocid 986: Clarity evaluation completed in 1.32 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 986: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 986\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1029\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.24 sec\u001b[0m\n",
      "\u001b[93mDocid 410: Non-NLI GPT metrics computed in 1.24 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=53 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.97 sec\u001b[0m\n",
      "\u001b[95mDocid 867: Clarity evaluation completed in 0.97 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 867: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 867\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 778\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  53%|██████████▌         | 127/240 [00:29<00:20,  5.61it/s, Last docid=866, Elapsed (min)=0.49]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.35 sec\u001b[0m\n",
      "\u001b[93mDocid 241: Non-NLI GPT metrics computed in 1.35 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=53 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.95 sec\u001b[0m\n",
      "\u001b[95mDocid 874: Clarity evaluation completed in 0.95 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 874: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 874\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 777\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.95 sec\u001b[0m\n",
      "\u001b[93mDocid 1035: Non-NLI GPT metrics computed in 0.95 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=33 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.32 sec\u001b[0m\n",
      "\u001b[95mDocid 866: Clarity evaluation completed in 1.32 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 866: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 866\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 791\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  53%|██████████▋         | 128/240 [00:29<00:28,  3.98it/s, Last docid=410, Elapsed (min)=0.49]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.19 sec\u001b[0m\n",
      "\u001b[93mDocid 781: Non-NLI GPT metrics computed in 1.19 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=31 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.29 sec\u001b[0m\n",
      "\u001b[95mDocid 243: Clarity evaluation completed in 1.29 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 243: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 243\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1047\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.97 sec\u001b[0m\n",
      "\u001b[95mDocid 410: Clarity evaluation completed in 0.97 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 410: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 410\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 785\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating docs:  54%|██████████▏        | 129/240 [00:30<00:27,  3.98it/s, Last docid=1035, Elapsed (min)=0.50]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.20 sec\u001b[0m\n",
      "\u001b[93mDocid 1029: Non-NLI GPT metrics computed in 1.20 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=22 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.32 sec\u001b[0m\n",
      "\u001b[93mDocid 782: Non-NLI GPT metrics computed in 1.32 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=38 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.21 sec\u001b[0m\n",
      "\u001b[93mDocid 778: Non-NLI GPT metrics computed in 1.21 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=33 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.01 sec\u001b[0m\n",
      "\u001b[95mDocid 1035: Clarity evaluation completed in 1.01 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1035: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1035\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 244\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  55%|██████████▉         | 131/240 [00:30<00:29,  3.64it/s, Last docid=241, Elapsed (min)=0.51]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.35 sec\u001b[0m\n",
      "\u001b[93mDocid 777: Non-NLI GPT metrics computed in 1.35 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=39 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.49 sec\u001b[0m\n",
      "\u001b[95mDocid 241: Clarity evaluation completed in 1.49 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 241: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 241\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 649\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.39 sec\u001b[0m\n",
      "\u001b[93mDocid 791: Non-NLI GPT metrics computed in 1.39 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=13 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  56%|███████████▎        | 135/240 [00:30<00:15,  6.76it/s, Last docid=781, Elapsed (min)=0.51]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.04 sec\u001b[0m\n",
      "\u001b[93mDocid 1047: Non-NLI GPT metrics computed in 1.04 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=23 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.77 sec\u001b[0m\n",
      "\u001b[95mDocid 778: Clarity evaluation completed in 0.77 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 778: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 778\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 648\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.84 sec\u001b[0m\n",
      "\u001b[95mDocid 782: Clarity evaluation completed in 0.84 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 782: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 782\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 783\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.89 sec\u001b[0m\n",
      "\u001b[95mDocid 1029: Clarity evaluation completed in 0.89 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1029: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1029\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 256\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.26 sec\u001b[0m\n",
      "\u001b[95mDocid 781: Clarity evaluation completed in 1.26 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 781: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 781\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 755\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.26 sec\u001b[0m\n",
      "\u001b[93mDocid 785: Non-NLI GPT metrics computed in 1.26 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=26 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.97 sec\u001b[0m\n",
      "\u001b[93mDocid 244: Non-NLI GPT metrics computed in 0.97 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=10 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  57%|███████████▍        | 137/240 [00:31<00:19,  5.35it/s, Last docid=777, Elapsed (min)=0.52]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.77 sec\u001b[0m\n",
      "\u001b[95mDocid 791: Clarity evaluation completed in 0.77 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 791: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 791\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 612\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.87 sec\u001b[0m\n",
      "\u001b[93mDocid 649: Non-NLI GPT metrics computed in 0.87 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=36 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.95 sec\u001b[0m\n",
      "\u001b[95mDocid 777: Clarity evaluation completed in 0.95 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 777: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 777\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 817\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  57%|██████████▉        | 138/240 [00:31<00:19,  5.22it/s, Last docid=1047, Elapsed (min)=0.53]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.88 sec\u001b[0m\n",
      "\u001b[95mDocid 1047: Clarity evaluation completed in 0.88 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1047: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1047\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 259\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.82 sec\u001b[0m\n",
      "\u001b[93mDocid 256: Non-NLI GPT metrics computed in 0.82 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=23 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  58%|███████████▌        | 139/240 [00:31<00:23,  4.35it/s, Last docid=244, Elapsed (min)=0.53]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.11 sec\u001b[0m\n",
      "\u001b[93mDocid 648: Non-NLI GPT metrics computed in 1.11 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=23 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.09 sec\u001b[0m\n",
      "\u001b[93mDocid 783: Non-NLI GPT metrics computed in 1.09 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=22 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.86 sec\u001b[0m\n",
      "\u001b[95mDocid 244: Clarity evaluation completed in 0.86 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 244: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 244\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 257\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  58%|███████████▋        | 140/240 [00:32<00:21,  4.60it/s, Last docid=785, Elapsed (min)=0.54]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.76 sec\u001b[0m\n",
      "\u001b[95mDocid 649: Clarity evaluation completed in 0.76 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 649: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 649\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 581\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.21 sec\u001b[0m\n",
      "\u001b[95mDocid 785: Clarity evaluation completed in 1.21 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 785: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 785\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 532\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.45 sec\u001b[0m\n",
      "\u001b[93mDocid 755: Non-NLI GPT metrics computed in 1.45 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=21 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  59%|███████████▊        | 142/240 [00:32<00:21,  4.55it/s, Last docid=256, Elapsed (min)=0.54]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 5.36 sec\u001b[0m\n",
      "\u001b[95mDocid 790: Clarity evaluation completed in 5.36 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 790: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 790\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 579\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.97 sec\u001b[0m\n",
      "\u001b[95mDocid 256: Clarity evaluation completed in 0.97 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 256: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 256\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 813\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.18 sec\u001b[0m\n",
      "\u001b[93mDocid 259: Non-NLI GPT metrics computed in 1.19 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=20 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  60%|████████████        | 145/240 [00:32<00:16,  5.65it/s, Last docid=648, Elapsed (min)=0.55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.99 sec\u001b[0m\n",
      "\u001b[95mDocid 783: Clarity evaluation completed in 0.99 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 783: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 783\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 415\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.13 sec\u001b[0m\n",
      "\u001b[95mDocid 648: Clarity evaluation completed in 1.13 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 648: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 648\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 814\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.66 sec\u001b[0m\n",
      "\u001b[93mDocid 612: Non-NLI GPT metrics computed in 1.66 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=23 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  61%|████████████▏       | 146/240 [00:33<00:17,  5.42it/s, Last docid=755, Elapsed (min)=0.55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 0.88 sec\u001b[0m\n",
      "\u001b[93mDocid 532: Non-NLI GPT metrics computed in 0.88 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=15 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.12 sec\u001b[0m\n",
      "\u001b[93mDocid 257: Non-NLI GPT metrics computed in 1.12 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=16 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.89 sec\u001b[0m\n",
      "\u001b[95mDocid 755: Clarity evaluation completed in 0.89 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 755: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 755\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 890\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  61%|████████████▎       | 147/240 [00:33<00:21,  4.40it/s, Last docid=259, Elapsed (min)=0.56]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.77 sec\u001b[0m\n",
      "\u001b[95mDocid 259: Clarity evaluation completed in 0.77 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 259: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 259\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 905\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.97 sec\u001b[0m\n",
      "\u001b[93mDocid 813: Non-NLI GPT metrics computed in 0.97 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=13 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.00 sec\u001b[0m\n",
      "\u001b[93mDocid 579: Non-NLI GPT metrics computed in 1.00 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=19 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  62%|████████████▎       | 148/240 [00:33<00:21,  4.32it/s, Last docid=612, Elapsed (min)=0.56]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.75 sec\u001b[0m\n",
      "\u001b[95mDocid 612: Clarity evaluation completed in 0.75 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 612: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 612\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 586\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.97 sec\u001b[0m\n",
      "\u001b[93mDocid 415: Non-NLI GPT metrics computed in 0.97 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=38 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 2.49 sec\u001b[0m\n",
      "\u001b[93mDocid 817: Non-NLI GPT metrics computed in 2.49 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=24 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.92 sec\u001b[0m\n",
      "\u001b[93mDocid 814: Non-NLI GPT metrics computed in 0.92 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=12 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  62%|████████████▍       | 149/240 [00:34<00:28,  3.19it/s, Last docid=532, Elapsed (min)=0.57]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 2.13 sec\u001b[0m\n",
      "\u001b[93mDocid 581: Non-NLI GPT metrics computed in 2.13 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=22 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.25 sec\u001b[0m\n",
      "\u001b[95mDocid 532: Clarity evaluation completed in 1.25 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 532: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 532\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 583\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  63%|████████████▌       | 151/240 [00:34<00:26,  3.34it/s, Last docid=813, Elapsed (min)=0.58]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.50 sec\u001b[0m\n",
      "\u001b[95mDocid 257: Clarity evaluation completed in 1.50 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 257: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 257\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 821\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.03 sec\u001b[0m\n",
      "\u001b[95mDocid 579: Clarity evaluation completed in 1.03 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 579: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 579\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 255\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.10 sec\u001b[0m\n",
      "\u001b[95mDocid 813: Clarity evaluation completed in 1.10 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 813: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 813\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 699\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.20 sec\u001b[0m\n",
      "\u001b[93mDocid 905: Non-NLI GPT metrics computed in 1.20 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=22 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  64%|████████████▊       | 153/240 [00:34<00:16,  5.28it/s, Last docid=415, Elapsed (min)=0.58]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.97 sec\u001b[0m\n",
      "\u001b[95mDocid 814: Clarity evaluation completed in 0.97 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 814: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 814\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 698\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.74 sec\u001b[0m\n",
      "\u001b[93mDocid 890: Non-NLI GPT metrics computed in 1.75 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=52 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.16 sec\u001b[0m\n",
      "\u001b[93mDocid 586: Non-NLI GPT metrics computed in 1.16 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=15 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.13 sec\u001b[0m\n",
      "\u001b[95mDocid 415: Clarity evaluation completed in 1.13 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 415: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 415\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 901\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.86 sec\u001b[0m\n",
      "\u001b[93mDocid 583: Non-NLI GPT metrics computed in 0.86 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=32 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  65%|████████████▉       | 155/240 [00:35<00:22,  3.79it/s, Last docid=581, Elapsed (min)=0.59]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 0.86 sec\u001b[0m\n",
      "\u001b[93mDocid 699: Non-NLI GPT metrics computed in 0.86 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=19 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.98 sec\u001b[0m\n",
      "\u001b[93mDocid 255: Non-NLI GPT metrics computed in 0.98 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=20 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.02 sec\u001b[0m\n",
      "\u001b[93mDocid 821: Non-NLI GPT metrics computed in 1.02 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=16 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.42 sec\u001b[0m\n",
      "\u001b[95mDocid 581: Clarity evaluation completed in 1.42 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 581: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 581\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 820\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating docs:  65%|████████████▉       | 155/240 [00:35<00:22,  3.79it/s, Last docid=586, Elapsed (min)=0.60]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.83 sec\u001b[0m\n",
      "\u001b[95mDocid 586: Clarity evaluation completed in 0.83 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 586: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 586\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 815\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  66%|█████████████▎      | 159/240 [00:36<00:14,  5.43it/s, Last docid=583, Elapsed (min)=0.60]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.11 sec\u001b[0m\n",
      "\u001b[95mDocid 890: Clarity evaluation completed in 1.11 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 890: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 890\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 894\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.34 sec\u001b[0m\n",
      "\u001b[95mDocid 905: Clarity evaluation completed in 1.34 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 905: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 905\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 530\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.00 sec\u001b[0m\n",
      "\u001b[95mDocid 583: Clarity evaluation completed in 1.00 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 583: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 583\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 654\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.25 sec\u001b[0m\n",
      "\u001b[93mDocid 901: Non-NLI GPT metrics computed in 1.25 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=50 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  67%|█████████████▍      | 161/240 [00:36<00:15,  5.19it/s, Last docid=255, Elapsed (min)=0.61]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.91 sec\u001b[0m\n",
      "\u001b[95mDocid 699: Clarity evaluation completed in 0.91 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 699: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 699\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 464\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.03 sec\u001b[0m\n",
      "\u001b[95mDocid 255: Clarity evaluation completed in 1.03 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 255: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 255\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 598\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  68%|█████████████▌      | 163/240 [00:36<00:12,  6.09it/s, Last docid=817, Elapsed (min)=0.61]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.07 sec\u001b[0m\n",
      "\u001b[95mDocid 821: Clarity evaluation completed in 1.07 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 821: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 821\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1051\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 2.97 sec\u001b[0m\n",
      "\u001b[95mDocid 817: Clarity evaluation completed in 2.97 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 817: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 817\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1050\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.08 sec\u001b[0m\n",
      "\u001b[93mDocid 815: Non-NLI GPT metrics computed in 1.08 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=14 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  68%|█████████████▋      | 164/240 [00:36<00:12,  6.28it/s, Last docid=901, Elapsed (min)=0.62]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.77 sec\u001b[0m\n",
      "\u001b[95mDocid 901: Clarity evaluation completed in 0.77 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 901: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 901\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1095\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.98 sec\u001b[0m\n",
      "\u001b[93mDocid 530: Non-NLI GPT metrics computed in 0.98 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=14 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 2.31 sec\u001b[0m\n",
      "\u001b[93mDocid 698: Non-NLI GPT metrics computed in 2.31 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=15 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.30 sec\u001b[0m\n",
      "\u001b[93mDocid 894: Non-NLI GPT metrics computed in 1.30 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=62 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.93 sec\u001b[0m\n",
      "\u001b[93mDocid 464: Non-NLI GPT metrics computed in 0.93 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=30 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.70 sec\u001b[0m\n",
      "\u001b[93mDocid 820: Non-NLI GPT metrics computed in 1.70 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=16 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.46 sec\u001b[0m\n",
      "\u001b[93mDocid 654: Non-NLI GPT metrics computed in 1.46 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=28 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.92 sec\u001b[0m\n",
      "\u001b[93mDocid 1050: Non-NLI GPT metrics computed in 0.92 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=27 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.15 sec\u001b[0m\n",
      "\u001b[93mDocid 598: Non-NLI GPT metrics computed in 1.15 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=19 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.00 sec\u001b[0m\n",
      "\u001b[95mDocid 815: Clarity evaluation completed in 1.00 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 815: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 815\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 799\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  69%|█████████████▊      | 165/240 [00:37<00:24,  3.07it/s, Last docid=815, Elapsed (min)=0.63]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.19 sec\u001b[0m\n",
      "\u001b[93mDocid 1051: Non-NLI GPT metrics computed in 1.19 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=25 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.90 sec\u001b[0m\n",
      "\u001b[93mDocid 1095: Non-NLI GPT metrics computed in 0.90 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=41 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  69%|█████████████▊      | 166/240 [00:38<00:23,  3.16it/s, Last docid=530, Elapsed (min)=0.64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.97 sec\u001b[0m\n",
      "\u001b[95mDocid 698: Clarity evaluation completed in 0.97 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 698: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 698\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 462\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.11 sec\u001b[0m\n",
      "\u001b[95mDocid 530: Clarity evaluation completed in 1.11 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 530: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 530\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1078\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  71%|██████████████▏     | 170/240 [00:38<00:13,  5.11it/s, Last docid=598, Elapsed (min)=0.64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.21 sec\u001b[0m\n",
      "\u001b[95mDocid 894: Clarity evaluation completed in 1.21 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 894: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 894\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1093\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.81 sec\u001b[0m\n",
      "\u001b[95mDocid 1050: Clarity evaluation completed in 0.81 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1050: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1050\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 802\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.92 sec\u001b[0m\n",
      "\u001b[95mDocid 598: Clarity evaluation completed in 0.92 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 598: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 598\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 811\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  72%|██████████████▍     | 173/240 [00:38<00:09,  7.17it/s, Last docid=654, Elapsed (min)=0.65]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 0.96 sec\u001b[0m\n",
      "\u001b[93mDocid 799: Non-NLI GPT metrics computed in 0.96 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=47 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.47 sec\u001b[0m\n",
      "\u001b[95mDocid 820: Clarity evaluation completed in 1.47 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 820: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 820\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1082\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.57 sec\u001b[0m\n",
      "\u001b[95mDocid 464: Clarity evaluation completed in 1.57 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 464: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 464\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1054\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.33 sec\u001b[0m\n",
      "\u001b[95mDocid 654: Clarity evaluation completed in 1.33 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 654: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 654\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 650\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  73%|█████████████▊     | 175/240 [00:39<00:09,  6.91it/s, Last docid=1095, Elapsed (min)=0.65]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 0.97 sec\u001b[0m\n",
      "\u001b[93mDocid 1078: Non-NLI GPT metrics computed in 0.97 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=35 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.38 sec\u001b[0m\n",
      "\u001b[95mDocid 1051: Clarity evaluation completed in 1.38 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1051: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1051\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1090\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.40 sec\u001b[0m\n",
      "\u001b[95mDocid 1095: Clarity evaluation completed in 1.40 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1095: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1095\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 651\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.15 sec\u001b[0m\n",
      "\u001b[93mDocid 462: Non-NLI GPT metrics computed in 1.15 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=27 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  73%|██████████████▋     | 176/240 [00:39<00:13,  4.62it/s, Last docid=799, Elapsed (min)=0.66]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.95 sec\u001b[0m\n",
      "\u001b[95mDocid 799: Clarity evaluation completed in 0.95 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 799: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 799\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 949\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.23 sec\u001b[0m\n",
      "\u001b[93mDocid 802: Non-NLI GPT metrics computed in 1.23 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=25 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.29 sec\u001b[0m\n",
      "\u001b[93mDocid 1093: Non-NLI GPT metrics computed in 1.29 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=36 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.05 sec\u001b[0m\n",
      "\u001b[93mDocid 650: Non-NLI GPT metrics computed in 1.05 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=17 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.36 sec\u001b[0m\n",
      "\u001b[93mDocid 1082: Non-NLI GPT metrics computed in 1.36 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=21 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  74%|██████████████     | 177/240 [00:40<00:18,  3.36it/s, Last docid=1078, Elapsed (min)=0.67]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.00 sec\u001b[0m\n",
      "\u001b[93mDocid 1090: Non-NLI GPT metrics computed in 1.00 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=18 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.58 sec\u001b[0m\n",
      "\u001b[93mDocid 811: Non-NLI GPT metrics computed in 1.58 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=40 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.21 sec\u001b[0m\n",
      "\u001b[95mDocid 1078: Clarity evaluation completed in 1.22 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1078: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1078\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1089\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.44 sec\u001b[0m\n",
      "\u001b[93mDocid 1054: Non-NLI GPT metrics computed in 1.44 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=25 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  74%|██████████████▊     | 178/240 [00:40<00:18,  3.44it/s, Last docid=462, Elapsed (min)=0.68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.29 sec\u001b[0m\n",
      "\u001b[93mDocid 651: Non-NLI GPT metrics computed in 1.29 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=16 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.32 sec\u001b[0m\n",
      "\u001b[95mDocid 462: Clarity evaluation completed in 1.32 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 462: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 462\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1084\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.97 sec\u001b[0m\n",
      "\u001b[93mDocid 949: Non-NLI GPT metrics computed in 0.97 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=22 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  75%|██████████████▏    | 179/240 [00:41<00:20,  2.93it/s, Last docid=1090, Elapsed (min)=0.69]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.85 sec\u001b[0m\n",
      "\u001b[95mDocid 1090: Clarity evaluation completed in 0.85 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1090: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1090\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 465\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  76%|███████████████▏    | 182/240 [00:41<00:17,  3.22it/s, Last docid=802, Elapsed (min)=0.69]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.95 sec\u001b[0m\n",
      "\u001b[95mDocid 1054: Clarity evaluation completed in 0.95 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1054: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1054\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 589\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.79 sec\u001b[0m\n",
      "\u001b[95mDocid 651: Clarity evaluation completed in 0.79 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 651: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 651\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 795\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.16 sec\u001b[0m\n",
      "\u001b[95mDocid 811: Clarity evaluation completed in 1.16 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 811: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 811\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 467\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.73 sec\u001b[0m\n",
      "\u001b[95mDocid 802: Clarity evaluation completed in 1.73 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 802: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 802\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 634\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  77%|██████████████▌    | 184/240 [00:41<00:09,  5.69it/s, Last docid=1082, Elapsed (min)=0.70]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 0.91 sec\u001b[0m\n",
      "\u001b[93mDocid 1084: Non-NLI GPT metrics computed in 0.91 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=25 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.22 sec\u001b[0m\n",
      "\u001b[93mDocid 1089: Non-NLI GPT metrics computed in 1.22 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=22 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.66 sec\u001b[0m\n",
      "\u001b[95mDocid 650: Clarity evaluation completed in 1.66 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 650: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 650\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 434\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.50 sec\u001b[0m\n",
      "\u001b[95mDocid 1082: Clarity evaluation completed in 1.50 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1082: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1082\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 432\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  78%|███████████████▌    | 186/240 [00:41<00:07,  6.79it/s, Last docid=949, Elapsed (min)=0.70]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.14 sec\u001b[0m\n",
      "\u001b[95mDocid 949: Clarity evaluation completed in 1.14 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 949: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 949\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 442\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.98 sec\u001b[0m\n",
      "\u001b[93mDocid 465: Non-NLI GPT metrics computed in 0.98 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=14 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.90 sec\u001b[0m\n",
      "\u001b[93mDocid 795: Non-NLI GPT metrics computed in 0.90 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=38 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  78%|██████████████▊    | 187/240 [00:42<00:14,  3.76it/s, Last docid=1084, Elapsed (min)=0.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.01 sec\u001b[0m\n",
      "\u001b[93mDocid 634: Non-NLI GPT metrics computed in 1.01 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=13 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.03 sec\u001b[0m\n",
      "\u001b[95mDocid 1089: Clarity evaluation completed in 1.03 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1089: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1089\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 145\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.91 sec\u001b[0m\n",
      "\u001b[93mDocid 432: Non-NLI GPT metrics computed in 0.91 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=18 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.10 sec\u001b[0m\n",
      "\u001b[95mDocid 1084: Clarity evaluation completed in 1.10 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1084: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1084\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 473\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.25 sec\u001b[0m\n",
      "\u001b[93mDocid 467: Non-NLI GPT metrics computed in 1.25 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=26 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.38 sec\u001b[0m\n",
      "\u001b[93mDocid 589: Non-NLI GPT metrics computed in 1.38 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=14 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.84 sec\u001b[0m\n",
      "\u001b[93mDocid 442: Non-NLI GPT metrics computed in 0.84 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=14 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  79%|███████████████▊    | 190/240 [00:43<00:12,  4.08it/s, Last docid=795, Elapsed (min)=0.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.93 sec\u001b[0m\n",
      "\u001b[95mDocid 465: Clarity evaluation completed in 0.93 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 465: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 465\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 342\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 3.22 sec\u001b[0m\n",
      "\u001b[95mDocid 1093: Clarity evaluation completed in 3.22 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1093: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1093\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 433\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.82 sec\u001b[0m\n",
      "\u001b[95mDocid 795: Clarity evaluation completed in 0.82 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 795: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 795\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 154\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.49 sec\u001b[0m\n",
      "\u001b[93mDocid 434: Non-NLI GPT metrics computed in 1.49 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=16 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  80%|████████████████    | 192/240 [00:43<00:10,  4.38it/s, Last docid=432, Elapsed (min)=0.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.10 sec\u001b[0m\n",
      "\u001b[95mDocid 634: Clarity evaluation completed in 1.10 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 634: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 634\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 158\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.03 sec\u001b[0m\n",
      "\u001b[93mDocid 473: Non-NLI GPT metrics computed in 1.03 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=15 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.09 sec\u001b[0m\n",
      "\u001b[95mDocid 432: Clarity evaluation completed in 1.09 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 432: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 432\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 136\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.17 sec\u001b[0m\n",
      "\u001b[93mDocid 145: Non-NLI GPT metrics computed in 1.17 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=14 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  81%|████████████████▏   | 194/240 [00:43<00:09,  4.90it/s, Last docid=467, Elapsed (min)=0.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.21 sec\u001b[0m\n",
      "\u001b[95mDocid 589: Clarity evaluation completed in 1.21 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 589: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 589\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 476\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.28 sec\u001b[0m\n",
      "\u001b[95mDocid 467: Clarity evaluation completed in 1.28 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 467: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 467\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 84\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.93 sec\u001b[0m\n",
      "\u001b[93mDocid 433: Non-NLI GPT metrics computed in 0.93 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=26 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  82%|████████████████▎   | 196/240 [00:44<00:08,  4.99it/s, Last docid=434, Elapsed (min)=0.74]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.08 sec\u001b[0m\n",
      "\u001b[93mDocid 154: Non-NLI GPT metrics computed in 1.08 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=16 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.12 sec\u001b[0m\n",
      "\u001b[95mDocid 434: Clarity evaluation completed in 1.12 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 434: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 434\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 151\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.31 sec\u001b[0m\n",
      "\u001b[93mDocid 342: Non-NLI GPT metrics computed in 1.31 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=17 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  83%|████████████████▌   | 199/240 [00:44<00:07,  5.85it/s, Last docid=145, Elapsed (min)=0.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.01 sec\u001b[0m\n",
      "\u001b[95mDocid 473: Clarity evaluation completed in 1.01 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 473: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 473\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 320\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.07 sec\u001b[0m\n",
      "\u001b[93mDocid 158: Non-NLI GPT metrics computed in 1.07 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=15 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 2.00 sec\u001b[0m\n",
      "\u001b[95mDocid 442: Clarity evaluation completed in 2.00 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 442: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 442\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 144\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.02 sec\u001b[0m\n",
      "\u001b[95mDocid 145: Clarity evaluation completed in 1.02 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 145: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 145\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 454\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.03 sec\u001b[0m\n",
      "\u001b[93mDocid 84: Non-NLI GPT metrics computed in 1.03 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=14 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.13 sec\u001b[0m\n",
      "\u001b[93mDocid 476: Non-NLI GPT metrics computed in 1.13 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=13 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  84%|████████████████▊   | 201/240 [00:45<00:08,  4.59it/s, Last docid=433, Elapsed (min)=0.76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.94 sec\u001b[0m\n",
      "\u001b[95mDocid 342: Clarity evaluation completed in 0.94 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 342: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 342\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 156\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.03 sec\u001b[0m\n",
      "\u001b[93mDocid 151: Non-NLI GPT metrics computed in 1.03 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=74 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.47 sec\u001b[0m\n",
      "\u001b[95mDocid 433: Clarity evaluation completed in 1.47 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 433: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 433\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 339\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating docs:  84%|████████████████▊   | 201/240 [00:45<00:08,  4.59it/s, Last docid=158, Elapsed (min)=0.76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.81 sec\u001b[0m\n",
      "\u001b[95mDocid 158: Clarity evaluation completed in 0.81 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 158: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 158\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 629\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.82 sec\u001b[0m\n",
      "\u001b[93mDocid 144: Non-NLI GPT metrics computed in 0.82 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=20 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.87 sec\u001b[0m\n",
      "\u001b[93mDocid 136: Non-NLI GPT metrics computed in 1.87 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=10 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  85%|████████████████▉   | 203/240 [00:45<00:08,  4.45it/s, Last docid=476, Elapsed (min)=0.77]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.85 sec\u001b[0m\n",
      "\u001b[95mDocid 476: Clarity evaluation completed in 0.85 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 476: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 476\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 474\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.21 sec\u001b[0m\n",
      "\u001b[93mDocid 454: Non-NLI GPT metrics computed in 1.21 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=17 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  85%|█████████████████▊   | 204/240 [00:46<00:09,  3.84it/s, Last docid=84, Elapsed (min)=0.77]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 0.90 sec\u001b[0m\n",
      "\u001b[93mDocid 156: Non-NLI GPT metrics computed in 0.90 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=15 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.51 sec\u001b[0m\n",
      "\u001b[93mDocid 320: Non-NLI GPT metrics computed in 1.51 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=20 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.32 sec\u001b[0m\n",
      "\u001b[95mDocid 84: Clarity evaluation completed in 1.32 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 84: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 84\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 319\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  85%|█████████████████   | 205/240 [00:46<00:09,  3.58it/s, Last docid=136, Elapsed (min)=0.78]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 0.94 sec\u001b[0m\n",
      "\u001b[93mDocid 629: Non-NLI GPT metrics computed in 0.94 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=32 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.12 sec\u001b[0m\n",
      "\u001b[93mDocid 339: Non-NLI GPT metrics computed in 1.12 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=17 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.06 sec\u001b[0m\n",
      "\u001b[95mDocid 136: Clarity evaluation completed in 1.06 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 136: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 136\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 567\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  86%|█████████████████▏  | 206/240 [00:46<00:09,  3.58it/s, Last docid=144, Elapsed (min)=0.78]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.35 sec\u001b[0m\n",
      "\u001b[95mDocid 151: Clarity evaluation completed in 1.35 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 151: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 151\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 159\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.18 sec\u001b[0m\n",
      "\u001b[95mDocid 144: Clarity evaluation completed in 1.18 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 144: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 144\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 377\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  87%|█████████████████▎  | 208/240 [00:46<00:06,  5.24it/s, Last docid=454, Elapsed (min)=0.78]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.02 sec\u001b[0m\n",
      "\u001b[93mDocid 474: Non-NLI GPT metrics computed in 1.02 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=13 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.95 sec\u001b[0m\n",
      "\u001b[95mDocid 454: Clarity evaluation completed in 0.95 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 454: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 454\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 77\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  88%|█████████████████▌  | 210/240 [00:47<00:06,  4.98it/s, Last docid=320, Elapsed (min)=0.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.13 sec\u001b[0m\n",
      "\u001b[95mDocid 156: Clarity evaluation completed in 1.13 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 156: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 156\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 116\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.24 sec\u001b[0m\n",
      "\u001b[95mDocid 320: Clarity evaluation completed in 1.24 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 320: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 320\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 89\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.95 sec\u001b[0m\n",
      "\u001b[95mDocid 339: Clarity evaluation completed in 0.95 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 339: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 339\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 197\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating docs:  88%|█████████████████▌  | 210/240 [00:47<00:06,  4.98it/s, Last docid=339, Elapsed (min)=0.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.23 sec\u001b[0m\n",
      "\u001b[93mDocid 319: Non-NLI GPT metrics computed in 1.23 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=40 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.99 sec\u001b[0m\n",
      "\u001b[93mDocid 377: Non-NLI GPT metrics computed in 0.99 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=11 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  88%|█████████████████▋  | 212/240 [00:47<00:05,  5.36it/s, Last docid=629, Elapsed (min)=0.80]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.29 sec\u001b[0m\n",
      "\u001b[95mDocid 629: Clarity evaluation completed in 1.29 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 629: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 629\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 81\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.17 sec\u001b[0m\n",
      "\u001b[93mDocid 567: Non-NLI GPT metrics computed in 1.17 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=43 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.23 sec\u001b[0m\n",
      "\u001b[93mDocid 159: Non-NLI GPT metrics computed in 1.23 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=16 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  89%|█████████████████▊  | 213/240 [00:48<00:06,  4.09it/s, Last docid=474, Elapsed (min)=0.80]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.27 sec\u001b[0m\n",
      "\u001b[95mDocid 474: Clarity evaluation completed in 1.27 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 474: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 474\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1034\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  90%|█████████████████▉  | 215/240 [00:48<00:05,  4.69it/s, Last docid=319, Elapsed (min)=0.81]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 4.30 sec\u001b[0m\n",
      "\u001b[95mDocid 154: Clarity evaluation completed in 4.30 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 154: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 154\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 335\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.03 sec\u001b[0m\n",
      "\u001b[95mDocid 319: Clarity evaluation completed in 1.03 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 319: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 319\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 345\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.33 sec\u001b[0m\n",
      "\u001b[93mDocid 89: Non-NLI GPT metrics computed in 1.33 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=17 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.57 sec\u001b[0m\n",
      "\u001b[93mDocid 116: Non-NLI GPT metrics computed in 1.57 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=9 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.15 sec\u001b[0m\n",
      "\u001b[93mDocid 81: Non-NLI GPT metrics computed in 1.15 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=29 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  90%|██████████████████  | 216/240 [00:48<00:06,  3.75it/s, Last docid=377, Elapsed (min)=0.82]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.06 sec\u001b[0m\n",
      "\u001b[95mDocid 159: Clarity evaluation completed in 1.06 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 159: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 159\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 4\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.29 sec\u001b[0m\n",
      "\u001b[95mDocid 377: Clarity evaluation completed in 1.29 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 377: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 377\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 709\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.52 sec\u001b[0m\n",
      "\u001b[93mDocid 197: Non-NLI GPT metrics computed in 1.52 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=18 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.94 sec\u001b[0m\n",
      "\u001b[93mDocid 1034: Non-NLI GPT metrics computed in 0.94 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=57 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  91%|███████████████████  | 218/240 [00:49<00:06,  3.18it/s, Last docid=81, Elapsed (min)=0.83]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.23 sec\u001b[0m\n",
      "\u001b[93mDocid 335: Non-NLI GPT metrics computed in 1.23 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=42 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.13 sec\u001b[0m\n",
      "\u001b[93mDocid 345: Non-NLI GPT metrics computed in 1.13 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=44 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.84 sec\u001b[0m\n",
      "\u001b[95mDocid 81: Clarity evaluation completed in 0.84 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 81: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 81\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 41\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 2.81 sec\u001b[0m\n",
      "\u001b[93mDocid 77: Non-NLI GPT metrics computed in 2.81 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=61 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  92%|█████████████████▍ | 220/240 [00:50<00:05,  3.87it/s, Last docid=1034, Elapsed (min)=0.84]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.93 sec\u001b[0m\n",
      "\u001b[95mDocid 197: Clarity evaluation completed in 0.93 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 197: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 197\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1046\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.96 sec\u001b[0m\n",
      "\u001b[95mDocid 1034: Clarity evaluation completed in 0.97 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1034: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1034\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 873\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.30 sec\u001b[0m\n",
      "\u001b[93mDocid 4: Non-NLI GPT metrics computed in 1.30 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=33 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.35 sec\u001b[0m\n",
      "\u001b[93mDocid 709: Non-NLI GPT metrics computed in 1.35 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=48 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  92%|██████████████████▌ | 222/240 [00:50<00:05,  3.50it/s, Last docid=567, Elapsed (min)=0.85]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.73 sec\u001b[0m\n",
      "\u001b[95mDocid 116: Clarity evaluation completed in 1.73 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 116: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 116\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 550\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 2.99 sec\u001b[0m\n",
      "\u001b[95mDocid 567: Clarity evaluation completed in 2.99 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 567: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 567\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 574\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.12 sec\u001b[0m\n",
      "\u001b[95mDocid 345: Clarity evaluation completed in 1.12 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 345: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 345\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 421\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  93%|███████████████████▌ | 224/240 [00:50<00:03,  5.15it/s, Last docid=77, Elapsed (min)=0.85]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.11 sec\u001b[0m\n",
      "\u001b[93mDocid 41: Non-NLI GPT metrics computed in 1.11 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=19 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.20 sec\u001b[0m\n",
      "\u001b[95mDocid 77: Clarity evaluation completed in 1.20 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 77: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 77\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 1032\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  94%|██████████████████▊ | 225/240 [00:51<00:02,  5.29it/s, Last docid=335, Elapsed (min)=0.85]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.85 sec\u001b[0m\n",
      "\u001b[95mDocid 4: Clarity evaluation completed in 0.85 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 4: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 4\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 149\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.04 sec\u001b[0m\n",
      "\u001b[93mDocid 873: Non-NLI GPT metrics computed in 1.04 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=34 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.49 sec\u001b[0m\n",
      "\u001b[95mDocid 335: Clarity evaluation completed in 1.49 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 335: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 335\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 7\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.32 sec\u001b[0m\n",
      "\u001b[93mDocid 1046: Non-NLI GPT metrics computed in 1.32 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=27 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  95%|██████████████████▉ | 227/240 [00:51<00:02,  6.24it/s, Last docid=709, Elapsed (min)=0.86]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.00 sec\u001b[0m\n",
      "\u001b[95mDocid 709: Clarity evaluation completed in 1.00 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 709: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 709\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 45\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 0.96 sec\u001b[0m\n",
      "\u001b[93mDocid 574: Non-NLI GPT metrics computed in 0.96 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=30 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.16 sec\u001b[0m\n",
      "\u001b[93mDocid 550: Non-NLI GPT metrics computed in 1.16 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=48 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.09 sec\u001b[0m\n",
      "\u001b[93mDocid 421: Non-NLI GPT metrics computed in 1.09 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=32 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  95%|███████████████████ | 228/240 [00:52<00:03,  3.54it/s, Last docid=873, Elapsed (min)=0.87]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.21 sec\u001b[0m\n",
      "\u001b[95mDocid 41: Clarity evaluation completed in 1.21 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 41: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 41\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 548\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.97 sec\u001b[0m\n",
      "\u001b[95mDocid 873: Clarity evaluation completed in 0.97 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 873: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 873\u001b[0m\n",
      "\u001b[92mStarting evaluation for docid: 547\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.02 sec\u001b[0m\n",
      "\u001b[93mDocid 149: Non-NLI GPT metrics computed in 1.02 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=70 words)\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.20 sec\u001b[0m\n",
      "\u001b[93mDocid 1032: Non-NLI GPT metrics computed in 1.20 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=28 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  96%|███████████████████▏| 230/240 [00:52<00:02,  4.32it/s, Last docid=574, Elapsed (min)=0.88]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.11 sec\u001b[0m\n",
      "\u001b[95mDocid 1046: Clarity evaluation completed in 1.11 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1046: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1046\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.72 sec\u001b[0m\n",
      "\u001b[95mDocid 574: Clarity evaluation completed in 0.72 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 574: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 574\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.29 sec\u001b[0m\n",
      "\u001b[93mDocid 45: Non-NLI GPT metrics computed in 1.29 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=9 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  97%|██████████████████▍| 233/240 [00:53<00:01,  3.93it/s, Last docid=1032, Elapsed (min)=0.89]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.15 sec\u001b[0m\n",
      "\u001b[95mDocid 421: Clarity evaluation completed in 1.15 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 421: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 421\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 1.05 sec\u001b[0m\n",
      "\u001b[93mDocid 548: Non-NLI GPT metrics computed in 1.05 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=34 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.10 sec\u001b[0m\n",
      "\u001b[95mDocid 1032: Clarity evaluation completed in 1.10 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 1032: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 1032\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  98%|████████████████████▌| 235/240 [00:53<00:01,  4.87it/s, Last docid=45, Elapsed (min)=0.89]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.55 sec\u001b[0m\n",
      "\u001b[95mDocid 550: Clarity evaluation completed in 1.55 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 550: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 550\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 0.85 sec\u001b[0m\n",
      "\u001b[95mDocid 45: Clarity evaluation completed in 0.85 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 45: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 45\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  98%|███████████████████▋| 236/240 [00:53<00:00,  4.88it/s, Last docid=149, Elapsed (min)=0.90]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mNon-NLI GPT metrics computed in 1.43 sec\u001b[0m\n",
      "\u001b[93mDocid 547: Non-NLI GPT metrics computed in 1.43 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=30 words)\u001b[0m\n",
      "\u001b[95mClarity evaluation completed in 1.55 sec\u001b[0m\n",
      "\u001b[95mDocid 149: Clarity evaluation completed in 1.55 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 149: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 149\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  99%|████████████████████▋| 237/240 [00:53<00:00,  5.19it/s, Last docid=89, Elapsed (min)=0.90]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 5.11 sec\u001b[0m\n",
      "\u001b[95mDocid 89: Clarity evaluation completed in 5.11 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 89: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 89\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs:  99%|███████████████████▊| 238/240 [00:54<00:00,  4.30it/s, Last docid=548, Elapsed (min)=0.90]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.09 sec\u001b[0m\n",
      "\u001b[95mDocid 548: Clarity evaluation completed in 1.09 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 548: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 548\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs: 100%|███████████████████▉| 239/240 [00:54<00:00,  3.57it/s, Last docid=547, Elapsed (min)=0.91]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 1.05 sec\u001b[0m\n",
      "\u001b[95mDocid 547: Clarity evaluation completed in 1.05 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 547: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 547\u001b[0m\n",
      "\u001b[93mNon-NLI GPT metrics computed in 3.63 sec\u001b[0m\n",
      "\u001b[93mDocid 7: Non-NLI GPT metrics computed in 3.63 sec\u001b[0m\n",
      "\u001b[96mStarting clarity evaluation for question (len=55 words)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating docs: 100%|██████████████████████| 240/240 [00:55<00:00,  4.31it/s, Last docid=7, Elapsed (min)=0.93]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mClarity evaluation completed in 0.92 sec\u001b[0m\n",
      "\u001b[95mDocid 7: Clarity evaluation completed in 0.92 sec\u001b[0m\n",
      "\u001b[96mLength metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[96mDocid 7: Length metric computed in 0.00 sec\u001b[0m\n",
      "\u001b[92mFinished evaluation for docid: 7\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vstrict_GPT</td>\n",
       "      <td>0.290394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wstrict_GPT</td>\n",
       "      <td>0.281435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Astrict_GPT</td>\n",
       "      <td>0.278775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V_GPT</td>\n",
       "      <td>0.349306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W_GPT</td>\n",
       "      <td>0.318398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A_GPT</td>\n",
       "      <td>0.315762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>grammar_score</td>\n",
       "      <td>4.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>succinctness_score</td>\n",
       "      <td>3.995833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>readability_score</td>\n",
       "      <td>4.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>L</td>\n",
       "      <td>28.408333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               metric      score\n",
       "0         Vstrict_GPT   0.290394\n",
       "1         Wstrict_GPT   0.281435\n",
       "2         Astrict_GPT   0.278775\n",
       "3               V_GPT   0.349306\n",
       "4               W_GPT   0.318398\n",
       "5               A_GPT   0.315762\n",
       "6       grammar_score   4.812500\n",
       "7  succinctness_score   3.995833\n",
       "8   readability_score   4.766667\n",
       "9                   L  28.408333"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = Evaluator(\n",
    "    nuggets_file = \"../../data/nuggets.csv\",\n",
    "    eval_embedding = False,\n",
    "    eval_nli=False,\n",
    "    eval_chatgpt_nli=False,\n",
    "    eval_non_nli=True,\n",
    "    eval_clarity=True,\n",
    "    eval_length=True,\n",
    "    num_threads=11,\n",
    "    seed=42,\n",
    "    enable_logging=True,\n",
    "    llm_version='gpt-4o'\n",
    ")\n",
    "\n",
    "\n",
    "passages_df = pd.read_csv(\"../augmented-generation/results/passages.csv\")[['docid', 'content', 'passage']]\n",
    "questions_df = pd.read_csv(\"../augmented-generation/results/wo_kpe.csv\")\n",
    "results_df = evaluator.evaluate_passages_and_questions(passages_df, questions_df)\n",
    "\n",
    "\n",
    "average_metrics = results_df.mean(numeric_only=True)\n",
    "average_metrics = average_metrics.iloc[1:].reset_index().rename(columns={'index': 'metric', 0: 'score'})\n",
    "average_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8baae5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "time_stamp = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "results_df['timestamp'] = time_stamp\n",
    "results_df.to_csv(f\"../../results_wo_kpe-{time_stamp}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48385297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vstrict_GPT</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wstrict_GPT</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Astrict_GPT</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V_GPT</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W_GPT</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A_GPT</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>grammar_score</td>\n",
       "      <td>4.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>succinctness_score</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>readability_score</td>\n",
       "      <td>4.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>L</td>\n",
       "      <td>28.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               metric  score\n",
       "0         Vstrict_GPT   0.29\n",
       "1         Wstrict_GPT   0.28\n",
       "2         Astrict_GPT   0.28\n",
       "3               V_GPT   0.35\n",
       "4               W_GPT   0.32\n",
       "5               A_GPT   0.32\n",
       "6       grammar_score   4.81\n",
       "7  succinctness_score   4.00\n",
       "8   readability_score   4.77\n",
       "9                   L  28.41"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_metrics.round(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
